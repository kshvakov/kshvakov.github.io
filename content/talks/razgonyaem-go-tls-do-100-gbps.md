---
title: "Разгоняем Go TLS до 100 Gbps: kTLS, zero-copy и грабли продакшена"
date: 2024-12-02
description: "Как из Go-сервиса выжать десятки гигабит в HTTPS: почему TLS ломает zero-copy, как работает kTLS, где выигрывает перенос шифрования в ядро, и что пришлось поменять в реальном продакшене."
tags: ["go", "golang", "tls", "https", "ktls", "zero-copy", "sendfile", "splice", "linux", "производительность", "cdn", "kinescope", "сеть"]
---

Когда вы раздаёте статический контент, всё кажется простым: файл на диске → `sendfile` → сеть. Но стоит включить HTTPS и попытаться выжать десятки гигабит на сервер, как выясняется: «дорого» не шифрование само по себе, а **копирования, буферы и накладные расходы на пути данных**.

Расскажу про практическую сторону задачи: почему TLS ломает **zero‑copy** (передачу без копирования), зачем нужен kTLS, что именно меняется в пути данных, где появляется реальный выигрыш и какие грабли встречаются в продакшене (включая неожиданный рост хендшейков). В конце покажу, что смотреть в `perf` и `pprof`, чтобы не оптимизировать вслепую.

«100 Gbps» в названии — не «предел технологии», а ориентир под **конкретное железо и сетевую конфигурацию**. На другой платформе (CPU, память, ядро, сетевые карты) тот же подход может дать заметно больше — и наоборот.

Видео доклада:

{{< youtube id="5TipVdPRQ7s" title="Разгоняем Go TLS до 100 Gbps с сервера / Кирилл Шваков" >}}

## TL;DR

- **На больших объёмах дорого не «шифрование», а накладные расходы вокруг него**: лишние копии, аллокации и буферы быстро упираются в память, кэш CPU и GC.
- **Zero‑copy в Linux (`sendfile`, `splice`) — ключ к пропускной способности**, но TLS в userspace ломает zero‑copy, потому что данные нужно шифровать.
- **kTLS переносит слой TLS‑записей (record layer) в ядро**: вы пишете в сокет незашифрованные данные, а ядро само формирует TLS‑записи и шифрует. Это возвращает zero‑copy (в первую очередь на отправке).
- **В Go kTLS «из коробки» нет**: нужно достать ключевой материал из хендшейка (например, `SetTrafficSecret` в TLS 1.3) и правильно настроить сокет.
- **Практика**: на прод‑трафике упирались примерно в **73 Гбит/с** на сервер при **~40 000 соединений**. Дальше ограничение задаёт платформа (сеть, память, ядро, драйверы).
- **kTLS почти не помогает, если вы *генерируете* контент в userspace**: максимальный эффект — когда вы **отдаёте «как есть»** (файлы/чанки) и можете включить zero‑copy.

## Контекст: почему это вообще стало проблемой

Kinescope — видеоплатформа. Мы обрабатываем видео и раздаём его через свою сеть доставки контента (CDN)[^cdn-background]. Внешних подключений — сотни тысяч, трафик — сотни гигабит. И в 2024–2025 году «нормальный» интернет‑трафик — это уже **HTTPS** почти везде.

Неприятный факт: одно дело — «HTTP раздаёт быстро», и другое — **HTTP внутри TLS** на больших объёмах. На мегабитах вы этого не замечаете. На десятках гигабит проблема становится очевидной.

## Как устроен TLS в первом приближении (без криптографии)

Криптографию здесь почти не трогаю — это отдельная большая тема. Важна инженерная механика:

1. **TCP‑соединение**.
2. **TLS‑хендшейк**: клиент и сервер договариваются о версии/шифрах и получают ключевой материал.
3. **Обмен данными**: приложение читает/пишет байты, а «внутри» происходит упаковка в TLS‑records и шифрование/расшифровка.

Использую два термина:

- **userspace** — код приложения в пространстве пользователя;
- **kernelspace** — ядро Linux и сетевой стек.

Ключевое слово здесь — *«внутри»*: от него зависит, будет ли zero‑copy.

### Что именно меняется, когда вы «включаете TLS»

Для раздачи больших файлов важно ответить на один вопрос: **где находятся данные в момент шифрования**.

#### 1) HTTP без TLS: zero‑copy «получается сам»

```text
file on disk
   |
   |  (sendfile)
   v
kernel (page cache / TCP stack)
   |
   v
NIC -> network -> client
```

В идеале приложение делает минимум: «склеивает» HTTP‑заголовки и говорит ядру: «вот файл, вот сокет».

#### 2) TLS в userspace: zero‑copy ломается из‑за шифрования

```text
file on disk
   |
   |  read()
   v
userspace (Go)
   |   encrypt (crypto/tls)
   v
userspace buffer
   |
   |  write()
   v
kernel (TCP stack)
   |
   v
NIC -> network -> client
```

Тут сразу появляются две неприятности: копирование «туда‑сюда» и аллокации под буферы.

#### 3) TLS с kTLS: возвращаем zero‑copy на отправке

```text
file on disk
   |
   |  (sendfile)  +  kTLS record layer in kernelspace
   v
kernel (TLS record + crypto + TCP stack)
   |
   v
NIC -> network -> client
```

Важное ограничение: kTLS особенно хорошо работает там, где вы реально можете использовать zero‑copy путь (например, отдаёте файлы/чанки, а не генерируете контент в памяти).

## Почему TLS становится проблемой на больших скоростях

Технически всё выглядит просто: устанавливается TCP‑соединение, проходит TLS‑хендшейк, дальше по соединению идут зашифрованные данные.

«Шифрование дорого» — фраза из разряда мемов. В реальности на типовых веб‑сценариях оно редко становится главной проблемой. Но как только вы начинаете передавать **большие объёмы данных**, цена резко растёт — и часто это цена не криптографии, а **копирования**:

- данные приходится поднимать в userspace;
- шифровать (что уже требует работы с буферами);
- снова отправлять в kernelspace;
- и всё это ещё взаимодействует с кэшем CPU и (в Go) с GC[^go-gc-problems].

На уровне ОС это видно так: вместо «протока» вы получаете множество циклов `read`/`write`, рост аллокаций и усиление давления на подсистему памяти. Дальше включается физика: **полоса памяти и кэш‑промахи** начинают ограничивать скорость раньше, чем CPU «в среднем по больнице».

## Почему иногда всё упирается не в CPU, а в память

Если вы раздаёте 70–100 Гбит/с, вы прогоняете через систему гигабайты в секунду. Любая лишняя копия данных «съедает» полосу памяти и кэш:

- одна лишняя копия → заметно дороже;
- две лишние копии → ещё дороже, иногда кратно;
- плюс управляющие структуры/буферы → аллокации, которые в Go могут втянуть GC.

Поэтому оптимизация «меньше копировать» часто даёт не «чуть быстрее», а ощутимый запас по производительности.

## Zero-copy в Linux и почему он так важен

В Unix‑системах есть системные вызовы, которые позволяют «перекидывать» данные, **не поднимая их в userspace**:

- `sendfile` — отправка данных из файла прямо в сокет;
- `splice` — передача данных между дескрипторами (например, из сокета в сокет) через pipe.

Когда zero‑copy работает, приложение почти «не участвует»: оно говорит ОС «вот дескриптор файла, вот сокет, отправь туда N байт», и дальше данные идут по более короткому пути.

Для нас это критично, потому что раздача — это не «несколько килобайт HTML», а **мегабайты и гигабайты**. (Подробнее о практическом применении zero‑copy в CDN — см. [«Раздача контента с HDD»]({{< relref "talks/razdacha-kontenta-s-hdd.md" >}}) и [«Нет времени объяснять, программируй!»]({{< relref "talks/net-vremeni-obyasnyat-programmiruy.md" >}}).)

### Где ломается zero‑copy

В TLS данные должны быть **зашифрованы**. Значит, классическое zero‑copy «как есть» не подходит: вы не можете просто «переложить» байты из файла в сокет — их нужно преобразовать.

Если TLS реализован в userspace, вы почти неизбежно приходите к схеме: прочитали данные в userspace, зашифровали в userspace, записали в сокет. Zero‑copy исчезает — и начинаются накладные расходы.

### Почему `io.Copy` — не гарантия zero‑copy в Go

Go умеет ускорять копирование через специальные интерфейсы, но это зависит от конкретных типов и реализации:

```text
io.Copy(dst, src)
  |
  +--> fast path (если src реализует WriteTo или dst реализует ReadFrom)
  |
  +--> slow path: цикл Read -> Write (буфер в userspace, аллокации/копии)
```

Поэтому, когда оптимизируете «раздачу файла», проверяйте не только CPU‑профили, но и то, каким путём реально пошло копирование. Как это увидеть — расскажу дальше.

## «Дешёвые клиенты» и шифры: сначала убедитесь, что проблема у вас есть

Перед тем как «чинить TLS», сначала **померьте**.

Мы добавили метрики по используемым шифрам (cipher suites) — профиль сильно зависит от клиентской базы. Часто часть клиентов выбирает шифры, которые на сервере работают дороже. Иногда это ещё и ломает аппаратное ускорение — и становится «вдруг дорого».

Так вы видите, что реально используется в проде, понимаете, какой процент трафика потенциально «дорогой», и уже потом решаете, что делать: менять политику шифров, менять версию TLS, включать/выключать конкретные варианты и т. п.

Дальше — решение уровня системной архитектуры.

## kTLS: идея «перенести TLS ближе к ядру»

**kTLS (kernel TLS)** — механизм, при котором часть TLS (record layer) переносится в kernelspace.

Идея в том, что **хендшейк** вы всё равно делаете обычной TLS‑библиотекой (в userspace). Но как только ключи получены, вы **передаёте их ядру**, включаете режим kTLS для сокета — и дальше работаете с соединением «как с обычным»: пишете в сокет незашифрованные данные, а ядро шифрует их само.

Если ядро умеет шифровать на отправке, оно может снова использовать zero‑copy‑механизмы и сократить копирования/аллокации.

### История идеи: Netflix/Facebook и внезапно Oracle

Забавный исторический момент: «новую» технологию обсуждали Netflix (FreeBSD) и Facebook (Linux), но если копнуть глубже, похожая идея была ещё раньше — у Oracle в Solaris.

Концепция «перенести TLS ближе к ядру/сетевому стеку ради производительности» — не «модная прихоть», а закономерное следствие желания убрать копирования.

### Дополнительный бонус: offload на сетевую карту

В некоторых сценариях TLS можно частично оффлоадить на NIC (TLS offload). На практике это сильно зависит от доступности железа и окружения — в РФ это может упираться даже в логистику/криптографию/сертификацию оборудования.

## Почему «просто включить kTLS в Go» не получилось

На уровне идеи всё просто: берём Go, включаем kTLS и радуемся. На практике есть два больших «но»:

1. **Нужно достать ключи** (traffic secrets) из TLS‑хендшейка и передать их ядру.
2. **Нужно поменять путь записи**: после включения kTLS в сокет нельзя продолжать писать туда уже зашифрованные TLS‑записи — нужно писать незашифрованные данные.

Поэтому в реальности мы сделали небольшой патч/форк TLS‑части, который:

- перехватывает ключевой материал (secrets) на этапе хендшейка;
- включает опции kTLS на сокете;
- и дальше позволяет использовать стандартные механизмы раздачи, включая путь, который даёт zero‑copy.

### Как это работает: незашифрованные данные и zero‑copy

Ключевой момент работы kTLS: после включения kTLS вы пишете в сокет **незашифрованные данные**, а record layer и шифрование становятся задачей ядра. Именно поэтому возвращается zero‑copy: приложение больше не нужно поднимать данные в userspace для шифрования — ядро делает это само, и данные могут идти напрямую из файла в сеть через `sendfile` или `splice`.

## Где выигрываем: `sendfile`, `splice` и «файлы как есть»

Важная деталь из Go‑мира: чтобы приложение действительно использовало zero‑copy, недостаточно «просто `io.Copy`».

В Go есть оптимизации по интерфейсам (`ReadFrom` / `WriteTo`), которые позволяют некоторым комбинациям Reader/Writer проваливаться в более эффективные системные вызовы.

Поэтому основной практический кейс, который действительно даёт эффект:

- **отдавать файл** (или большой кусок файла) в соединение **так, чтобы путь записи мог стать `sendfile`**;
- а шифрование делегировать ядру (kTLS).

Именно на этом сочетании получается «снова zero‑copy, но уже поверх TLS».

### Почему HTTP‑раздача хорошо ложится на `sendfile`

В классическом HTTP ответ устроен просто: сначала заголовки, потом тело. И если тело — это файл (или большой диапазон файла), то очень хочется сделать:

- заголовки отправить обычной записью;
- тело — `sendfile`.

Именно это и даёт большую часть выигрыша: приложение перестаёт быть «конвейером байтов».

## Результаты на продакшене (и почему они такие)

Ориентиры уровня:

- целевая нагрузка на сервер: десятки гигабит, с запасом;
- в одном из прогонов: **~73 Гбит/с** на прод‑трафике при **~40 000 соединений**;
- при этом приложение почти перестаёт быть «местом, где горит CPU» — дальше начинают определять результат сеть, память, ядро и окружение.

Потолок здесь задаёт не «Go» и не «kTLS», а **конкретная платформа** (полоса памяти, CPU, сеть, драйверы, версия ядра, баланс IRQ/softirq, конфигурация NIC). На другом железе вы упрётесь в другие лимиты и можете увидеть как меньшие, так и значительно большие числа.

Как правильно прочитать эти цифры:

- это не «магия kTLS»;
- это сумма множества вещей: zero‑copy путь, правильная работа с буферами, устранение лишних копий, и то, что делегирование в ядро убирает массу накладных расходов на уровне runtime/GC.

## Хендшейк тоже дорогой: сертификаты и возобновление сессии

Когда вы снижаете стоимость «передачи данных», на первый план начинают выходить другие расходы — в частности, **TLS‑хендшейк**.

Два практических шага:

### Уход от RSA‑сертификатов к ECDSA

Если у вас в цепочке всё ещё RSA‑сертификаты, то для части CPU это может быть заметно дороже, чем ECDSA. У вас RSA — просто замените сертификаты.

Порядок цифр из простого бенчмарка: на RSA хендшейк условно «в несколько раз» дороже, чем на эллиптических кривых (естественно, числа зависят от CPU и окружения, важен именно порядок).

### Возобновление сессии (session resumption) и общий ключ тикетов

Go (и браузеры) умеют возобновлять TLS‑сессии, что уменьшает цену повторных соединений. Но если у вас много серверов за одним доменом и запросы одного клиента приходят на разные машины, то для эффективного resumption нужно синхронизировать «секрет» (ticket key) между серверами.

Мы решили это просто: задали общий ключ (в рамках CDN‑контента это допустимый компромисс, потому что речь не про «секретные данные», а про ускорение раздачи публичного контента).

### Неочевидная причина всплеска хендшейков: HTTP 400+ и «умные» браузеры

Практичная деталь: если вы отдаёте видео и легитимно используете **коды ответа 400+** (например, для манифестов/управления), браузеры могут вести себя «слишком умно»: закрывать соединение и открывать новое.

С точки зрения TLS это выглядит как резкий рост количества хендшейков — и если хендшейк у вас дорогой, то «внезапно горит» именно он.

#### Как «легитимные 4xx» превращаются в «шторм хендшейков»

```text
client request #1  --->  server responds 4xx/5xx
      |
      +--> browser closes connection (чтобы «быстрее/надёжнее»)
               |
               +--> new TCP connection
                       |
                       +--> new TLS handshake
                               |
                               +--> request #2 (и так по кругу)
```

Если на этом пути у вас дорогой full handshake и нет устойчивого resumption между серверами, вы увидите «странные» пики CPU и задержек без видимого роста полезного трафика.

## Как измерять: `perf` и `pprof` (что смотреть и как интерпретировать)

Практический минимум, который помогает понять, *где вы теряете скорость*: в криптографии, в копированиях, в GC, в системных вызовах, в ядре или в сетевом стеке.

### База: какие метрики собрать до профилирования

- **TLS**:
  - доля **full handshake** vs **session resumption**;
  - распределение **cipher suites**;
  - p50/p95/p99 времени хендшейка;
  - RPS хендшейков и корреляция с 4xx/5xx.
- **Приложение**:
  - RPS, p95/p99 задержки, ошибки;
  - аллокации/сек, размер кучи (heap), паузы/CPU GC;
  - количество активных соединений.
- **ОС/железо**:
  - загрузка CPU по ядрам;
  - память: bandwidth/LLC misses (если есть доступ к расширенным счётчикам);
  - сеть: drops/softnet, IRQ нагрузка.

### pprof: быстрый ответ «где горит Go‑код»

Включите `net/http/pprof` (если ещё не включено) и снимайте профили в момент пика.

Что смотреть в CPU‑профиле:

- `crypto/tls` / `crypto/*` — если хендшейк/шифрование реально доминирует;
- `runtime.mallocgc`, `runtime.gcBgMarkWorker`, `runtime.scanobject` — если «побеждает» GC;
- `io.copyBuffer` / `bufio` / ваши циклы `Read`/`Write` — если вы реально гоняете байты через userspace.

Что смотреть в профилях alloc/heap:

- крупные аллокации на пути «ответ → запись в сокет»;
- буферы, которые можно переиспользовать (pool) или убрать, вернувшись к zero‑copy.

Полезные команды:

```bash
# CPU профиль на 30 секунд:
go tool pprof -http=:0 http://127.0.0.1:6060/debug/pprof/profile?seconds=30

# Heap:
go tool pprof -http=:0 http://127.0.0.1:6060/debug/pprof/heap

# Allocations:
go tool pprof -http=:0 http://127.0.0.1:6060/debug/pprof/allocs
```

### `perf`: быстрый ответ «где горит система/ядро»

Если `pprof` показывает «всё нормально», а сервер всё равно упирается — часто это означает, что ограничение уже вне Go‑кода: системные вызовы, ядро, сеть, память.

#### `perf top`: посмотреть «кто в топе» прямо сейчас

```bash
sudo perf top
```

Примеры интерпретаций:

- много времени в `crypto_*` → реальная цена криптографии;
- много времени в `copy_user_*`, `memcpy*` → вы копируете слишком много;
- много времени в `tcp_sendmsg`, `skb_*`, `ip_*` → упираетесь в сетевой стек;
- много времени в `ksoftirqd/*` / обработчиках IRQ → возможно, сетевые прерывания и softirq становятся узким местом.

#### `perf record`/`perf report`: снять профиль на интервал и спокойно разобрать

```bash
# профиль всей системы на 30 секунд
sudo perf record -F 99 -a -- sleep 30
sudo perf report
```

Если нужно сузить на процесс:

```bash
sudo perf record -F 99 -p <PID> -- sleep 30
sudo perf report
```

#### `perf stat`: быстро проверить «копии/кэш/ветвления» (если доступно)

```bash
sudo perf stat -p <PID> -- sleep 10
```

Это помогает увидеть, не упираетесь ли вы в кэш‑промахи/ветвления/инструкции на байт полезной работы. На «протоке» это часто важнее «процента CPU».

### Как понять, что у вас реально нет zero‑copy

Сигналы, которые обычно совпадают:

- pprof CPU показывает много `io.copyBuffer`/циклов Read→Write;
- allocs/heap растут с трафиком на отдаче;
- `perf top` показывает `copy_user_*`/`memcpy*` в заметной доле.

Если вы ожидаете `sendfile`/`splice`, а видите картину выше — значит, вы всё ещё «конвейер байтов», и TLS/Go/GC будут болеть при росте трафика.

## Важные ограничения и грабли

- **kTLS не ускорит генерацию контента**: если вы генерируете данные в памяти, то вы всё равно делаете работу в userspace, и выигрыш будет небольшим или его не будет.
- **Версии ядра имеют значение**: в разных версиях Linux у kTLS были разные возможности и баги; в продакшене иногда приходится сознательно использовать только часть функциональности (например, только отправку).
- **HTTP/2/QUIC**: на уровне «много трафика» часто хочется уйти в QUIC/HTTP3, но для массовой раздачи больших файлов это может снова упереться в копирования/модель исполнения и потребовать другого подхода.

## Чеклист: если вы захотите повторить

- **Метрики до оптимизаций**:
  - распределение cipher suites;
  - доля full handshake vs resumption;
  - время хендшейка по перцентилям;
  - RPS хендшейков (и корреляция с 4xx/5xx и пиками трафика).
- **Сначала «дешёвые победы»**:
  - если есть RSA — подумать про ECDSA;
  - включить и стабилизировать resumption между серверами.
- **Потом — архитектура данных**:
  - добиться реального zero‑copy (`sendfile`/`splice`) на «обычном HTTP»;
  - убрать лишние копии и аллокации на пути «файл → сеть».
- **И только потом — kTLS**:
  - научиться корректно отдавать ключи ядру;
  - убедиться, что после включения kTLS вы пишете незашифрованные данные;
  - прогонять на ядрах/драйверах, которые вы реально используете в проде.

## Итоги

Главная мысль простая: на очень больших скоростях у вас «дорогой» не TLS как концепция, а **всё, что заставляет данные лишний раз проходить через память приложения**.

Если ваш кейс — это раздача больших объёмов данных «как есть» (файлы/чанки), то связка:

- **zero‑copy (sendfile/splice)** + **kTLS**

может дать большой и очень «материальный» выигрыш.

Если вы генерируете контент в userspace, то kTLS — это не серебряная пуля: сначала нужно оптимизировать сам путь данных и аллокации.

## Сноски

[^cdn-background]: Подробнее о том, как мы строили собственную CDN и почему это было необходимо, см. [«Нет времени объяснять, программируй!»]({{< relref "talks/net-vremeni-obyasnyat-programmiruy.md" >}}) и [«Раздача контента с HDD»]({{< relref "talks/razdacha-kontenta-s-hdd.md" >}}).

[^go-gc-problems]: О проблемах с GC в Go при работе с большими объёмами данных и индексами в памяти см. [«Раздача контента с HDD»]({{< relref "talks/razdacha-kontenta-s-hdd.md" >}}).


