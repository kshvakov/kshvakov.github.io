---
title: "Раздача контента с HDD: быстро, увлекательно и надёжно"
date: 2021-09-20
description: "Практический опыт построения CDN на обычных HDD: как решить проблему миллиардов мелких файлов, оптимизировать работу с дисками и достичь высокой производительности при низкой стоимости."
tags: ["go", "golang", "cdn", "storage", "производительность", "hdd", "zero-copy", "tls", "kinescope", "инфраструктура", "доставка"]
---

Расскажу о нашем опыте построения CDN для раздачи видеоконтента на обычных жёстких дисках. Когда мы столкнулись с проблемой миллиардов мелких файлов и достигли предела производительности на уровне 4 гигабит на сервер, пришлось переосмыслить подход к хранению и раздаче данных.

{{< youtube id="M5IHpP9AhLg" title="Раздача контента с HDD: быстро, увлекательно и надёжно / Кирилл Шваков" >}}

## TL;DR

- **Проблема миллиардов мелких файлов решается упаковкой** — все данные хранятся в больших партициях (контейнерах), а не как отдельные файлы в файловой системе
- **Отказ от удаления файлов** — вместо удаления используется кольцевой буфер с перезаписью, что исключает фрагментацию и нагрузку от операций удаления
- **HDD могут быть эффективны** — при правильной организации данных (последовательная запись, batch-операции) можно достичь хорошей производительности
- **Zero-copy критичен для производительности** — использование `splice` и `sendfile` позволяет избежать копирования данных в user space
- **TLS в Go медленный** — встроенный TLS не поддерживает zero-copy, поэтому на больших объёмах трафика это заметно бьёт по производительности
- **GC создаёт проблемы** — при работе с большими индексами в памяти (десятки миллионов объектов) garbage collector становится узким местом
- **Метрики и профилирование обязательны** — без них невозможно понять, что происходит в системе и где реальные проблемы

## Вступление: откуда взялась проблема

Мы работаем над сетью доставки контента и видеоплатформой. У нас есть собственный плеер, и мы можем выбирать сервер, с которого будем раздавать видео. Долгое время это хорошо работало и масштабировалось — без заметных проблем.

Но к нам пришли клиенты, которые хотели использовать только CDN — они не хотели пользоваться другими нашими услугами. Соответственно, мы использовали их как origin. Origin — это может быть целый кластер с огромным количеством данных.

Проблемы начались, когда мы достигли примерно 4 гигабит на сервер — мы упали, и упала часть сети, которая раздавала контент пользователям.

### В чём были проблемы

Основная проблема — огромное количество файлов. На сервере их было миллиарды. Соответственно:

- Не работал кэш открытых файлов — файлы постоянно открывались и закрывались
- Постоянно происходило чтение с разных кусков диска — супер неудобно
- Мы постоянно что-то писали и постоянно что-то удаляли
- Кэш, который работал, нам ничего не помогал — как он работает, было непонятно

Нужно было с этим что-то делать.

## Требования к системе

Мы составили план того, что хотим от сервера. Требования были достаточно простые:

- **Хранить больше 100 терабайт данных** — это был минимум
- **Логика доступа к контенту** — подписи, время жизни, различные лимиты
- **Больше метрик, чем даёт nginx** — нам нужна была детальная статистика
- **Дешёвые логи** — данных постоянно пишем очень много, хотелось их оптимизировать
- **Хранить больше информации о контенте** — метаданные, которые нам нужны
- **Горизонтально масштабироваться** — один сервер не может хранить всю информацию origin'а клиента
- **Дешёвое решение** — обычные жёсткие диски, которые крутятся, никакого NVMe

Мы уже писали на Go и хорошо его знали. Сборка, оркестрация, мониторинг и логи были налажены — это не было проблемой. Стало понятно, что такой сервер нужен, и дальше мы начали разбираться с мелкими файлами.

## Решение проблемы с мелкими файлами

Первое, во что мы упёрлись, — мелкие файлы. От них нужно было избавиться. Мы стали создавать на диске несколько больших партиций и складывать данные внутрь. Физически это два файла:

1. **Файл метаданных** — фиксированного размера: ключ/идентификатор, смещение, флаги (например, «удалён»), checksum и т.п.
2. **Файл данных** — «контейнер», куда последовательно пишется контент

Плюс рядом храним дополнительную метаинформацию (в том числе в protobuf), чтобы потом не «выдирать» её из URL/домена и не ходить за ней в другие системы. На этом этапе проблему мелких файлов мы, по сути, закрыли.

## Работа с дисками: быстрые и обычные

Мы ввели два класса носителей: **быстрые** (SSD/NVMe) и **обычные** (HDD).

### Общая очередь на запись

У нас есть общая очередь на запись. На каждый диск — свой воркер (горутина): кто свободен, тот берёт следующую задачу и пишет на диск. Так запись получается более ровной и предсказуемой.

### Эвристика для мелких и больших файлов

У нас есть простая эвристика: «крупные» объекты сразу пишем на HDD (порог порядка ~225 МБ), а мелочь — сначала на SSD.

SSD дороже, и при огромном количестве мелких объектов/операций там тоже начинаются свои проблемы — особенно на random I/O.

### Merge: склеивание файлов

Те самые «мелкие» — это часто куски одного и того же видео (фрагменты). Если оставить их как есть, мы получаем много случайного чтения и деградацию доступа.

Что мы делаем: периодически «склеиваем» (merge) эти куски в последовательные сегменты. Для этого собираем статистику: где лежит кусок, на каком диске, в каком контейнере — и стараемся выстроить порядок так, чтобы дальше читать/писать максимально последовательно.

Если какое-то видео начинают активно смотреть, мы берём соответствующие фрагменты, упорядочиваем и переписываем на HDD уже «как нормальный файл» — одним последовательным куском.

В результате мы существенно (примерно на порядок) снизили random I/O на дисках.

## Удаление файлов: кольцевой буфер

Отдельная боль — удаление. В «классических» схемах (например, с nginx как proxy‑cache) есть постоянный churn: что-то записали, что-то вытеснили, что-то удалили. И это тоже создаёт нагрузку на диск.

Мы пошли по другому пути: **мы не удаляем файлы вовсе**. Есть партиции (контейнеры) на диске, и когда место заканчивается, мы просто сдвигаем указатель записи в начало и начинаем перезапись по кругу — по сути, кольцевой буфер.

У такого подхода есть два приятных эффекта:

1. **Смещение у нас фиксированное** — физически файлы остаются, то есть мы их не теряем
2. **Мы продолжаем читать, пока мы их не перетёрли** — поэтому не получается такого, что когда мы освобождаем место, всё сразу каша и становится

С дисками вроде бы всё более-менее нормально.

## Раздача контента

Файл ещё нужно отдать. Упрощённая схема выглядит так: есть интернет, к нам приходит запрос через DNS, он доходит до нашего сервера, и он его отдаёт.

Мы используем HTTP/1.1 — для раздачи видео в нашем случае он отлично подходит и достаточно прост. Поэтому мы написали свой веб‑сервер и «поженили» его с дисковым хранилищем.

### Шардирование данных

Так как мы не можем уместить весь контент клиента в один сервер, данные **шардируются**. Запрос может прийти на любой edge‑сервер группы.

Дальше мы строим ключ и понимаем, на каком сервере объект «должен» лежать. Если он есть локально — отдаём с локального диска. Если нет — проксируем запрос на нужный сервер и всё равно отдаём клиенту.

### Доступ к файлу

Как это выглядит внутри: пришёл запрос, мы быстро делаем lookup по индексу (где лежит объект, какое смещение и т.п.), а затем читаем данные из контейнера.

Индекс держим в памяти (поднимаем при старте). На чтении мы в ряде мест используем обычный `read` — до «идеального» варианта ещё есть, что полировать.

### Кэширование

Чтобы не упираться в диск на каждом запросе, мы держим кэш в памяти на каждом edge — читать с диска «в лоб» слишком дорого.

Размер кэша порядка десятков гигабайт (около 30 ГБ). Политика простая — LRU: она хорошо закрывает «горячий» контент.

Бывают сценарии, когда трафик резко подскакивает до сотен гигабит (например, из‑за рекламных кампаний). В такие моменты кэш особенно важен.

Параллельно мы считаем статистику. Часть «умной» логики предзагрузки кэша вынесена на отдельный контроллер: он собирает сигналы, формирует список ключей и подсказывает, что стоит положить в память. Это полезно, но работает не так идеально, как хотелось бы — мы честно это признаём.

## Что получилось

Типовая конфигурация сервера у нас сейчас: 32 ядра, около 195 ГБ памяти и два сетевых адаптера.

Исторически у нас было много SSD «с запасом», но со временем выяснилось, что их можно использовать меньше: сейчас на SSD приходится около 20% чтения, остальное — HDD.

Нагрузка гуляет по времени и по клиентам, но в целом всё «едет». Иногда мы упирались примерно в 50 Гбит/с на сервер — дальше просто забивается сеть, и диск уже ни при чём.

Но, конечно, не всё идеально — и дальше как раз про то, где мы «обожглись».

## Проблема номер один: TLS

Мы много пишем на Go и хорошо его знаем. Но на этой задаче Go местами «подложил свинью».

Первая проблема, с которой мы столкнулись — это TLS. Сейчас практически весь трафик шифруется, редко где можно встретить незашифрованный трафик. Соответственно, нужно обрабатывать TLS-запросы.

В Go есть встроенный TLS — у него отличный API, который нас вполне устраивает: мы можем менять сертификаты на лету, даже без остановки сервера. Но по производительности он не очень быстрый.

Мы стали смотреть, чем можно терминировать TLS. **Hitch** (проект Varnish) делает одну вещь — принимает TLS, расшифровывает и передаёт дальше (в TCP или Unix socket).

Дальше важная деталь про проксирование: если между двумя соединениями мы можем использовать `splice`, то данные почти не попадают в user space — и это сильно экономит CPU/память[^zero-copy-splice].

Но если мы упираемся в классический `read`/`write` (например, на Unix socket), данные начинают гоняться через user space — и это уже ощутимо дороже.

Отдельная надежда — **kTLS**: ядро умеет часть работы по TLS брать на себя. В Linux это появилось относительно недавно (например, в районе 5.3), и это потенциально может заметно ускорить терминацию[^ktls-details].

## Проблема номер два: Garbage Collector

Вторая проблема — GC. В Go garbage collector в целом хороший, но на очень больших объёмах объектов в памяти он становится фактором.

Когда мы строили индекс, сначала «по‑умному» сделали дерево.

Когда дошли до десятков миллионов файлов на сервер (порядка 50–80 млн), стало видно, что значимая часть CPU уходит на работу GC.

В итоге самое рабочее решение оказалось простым: перейти на обычный `map`. Да, это дороже по памяти, но заметно разгружает CPU.

Из‑за GC мы не можем бесконечно «набивать» память мелкими объектами под кэш: в какой-то момент GC начинает доминировать.

Ещё один эффект: из‑за накладных расходов модели памяти в Go мы в среднем тратим заметно больше RAM, чем хотелось бы. В результате страдает **page cache** — а в Linux это очень полезная штука для ускорения чтения. Это связано и с тем, что у нас нет `O_DIRECT`.

## Проблема номер три: предсказание нагрузки

Третья проблема — предсказание и предзагрузка.

Как я уже говорил, часть логики вынесена на отдельный сервер с контроллерами, которые считают статистику и подсказывают, что прогревать в кэше.

На бумаге всё выглядело логично: есть данные — значит, можно спрогнозировать, что пользователь будет смотреть, и заранее положить это в память.

На практике это работает хуже, чем хочется: угадывать поведение пользователя по одному‑двум событиям — неблагодарная задача.

В итоге мы часто загружаем в память больше данных, чем реально нужно, и часть из них потом не используется.

Понятно, что эту часть нужно переделывать — и планы на это есть.

## Планы на будущее

### Улучшение merge

У нас есть фаза merge, когда мы собираем куски в более крупные сегменты на основе статистики. Это всё хорошо, но нам нужно накопить достаточно большой промежуток времени, чтобы понять, что именно стоит склеивать. Обычно этот процесс запускается раз в несколько часов — и это долго.

Плюс сам merge даёт большой объём перезаписи. Мы «растягиваем» его во времени, но всё равно в этот момент диск заметно нагружается — и это видно в метриках (например, в iowait).

Если перестроить процесс, можно будет «сливать» куски быстрее и дешевле — понимание, как именно, у нас есть.

Одна из идей — не писать часть мелочи на SSD, а короткое время держать её в памяти, собирать более крупными блоками и уже ими последовательно писать на HDD.

### План номер два: переход на Rust

Так как у нас есть достаточно большие проблемы с direct I/O и page cache, мы не можем использовать память так эффективно, как хотелось бы. Есть план разобраться с TLS, и есть план идти в сторону io_uring — нативного асинхронного I/O в Linux (и для TCP, и для файлов).

Мы хотели держать в памяти около 30 ГБ кэша, но по факту потребление получается заметно выше (60+ ГБ) — из‑за накладных расходов рантайма и GC.

Плюс накопился примерно год опыта эксплуатации этого сервиса в продакшене — стало понятнее, что именно в структуре хранения и в merge‑процессах хочется поменять.

И мы понимаем, что SSD, которые у нас сейчас есть, по объёму можно сокращать — возможно, существенно, если научиться нормально работать с памятью и I/O. Но для этого нам, скорее всего, придётся менять Go на что-то другое.

## Что почитать

Что почитать:

- статьи ScyllaDB про доступ к данным и работу с диском в Linux (в том числе про `mmap` и его trade‑off);
- статьи Cloudflare про сеть и высоконагруженные системы;
- материалы про zero‑copy в Linux (`splice`, `sendfile`) и современные варианты на базе BPF/сокетного слоя.

Отдельный вывод автора: «всё проверять руками» — бенчмарки и обещания «у нас всё летает» часто не выдерживают встречи с реальной нагрузкой.

## Вопросы и ответы

### Про работу с HDD и очередь запросов

**Вопрос**: У HDD есть предел по параллельным чтениям: если «перегнуть», латентность резко растёт. Вы это как-то ограничиваете (эвристикой/динамикой) или полагаетесь на кэш/раскладку?

**Ответ**: Отдельной «умной» эвристики на уровне диска сейчас нет: многое закрывается кэшем и тем, как мы раскладываем данные.

По профилю контента у нас примерно 70% — это то, что смотрят каждый день (условно «тёплое/горячее»), и оно всё равно будет регулярно запрошено. Классический TTL‑кэш «на сутки» для такого профиля работает плохо, поэтому мы держим значимый объём данных в памяти (DRAM) и практически не вытесняем то, что долго остаётся востребованным.

Для чтения есть промежуточный кэш в памяти плюс нам сильно помогает page cache ОС. Сам по себе HDD в «чистом» виде отдаёт порядка сотен мегабайт в секунду (то есть меньше 1 Гбит/с), но за счёт кэшей и последовательного доступа на практике можно увидеть и большие цифры.

Ещё помогает раскладка: мы стараемся класть куски одного файла на один и тот же диск. Тогда при всплеске нагрузки обычно «горит» один диск (он справляется), а остальные запросы распределяются по другим. Сценарий «слишком много горячего на одном диске» случается редко — «пока везёт».

**Вопрос**: То есть «везёт» — это тоже логика раскладки?

**Ответ**: Да: мы стараемся класть части одного файла на один диск. Это повышает шанс, что в моменте не получится так, что один диск станет точкой концентрации сразу по многим популярным видео.

### Про размеры партиций и структуру хранения

**Вопрос**: Правильно ли я понял: вы «упаковываете» мелкие файлы в крупные контейнеры? Контейнер фиксированного размера?

**Ответ**: Да, размер фиксирован: партиции создаются при инициализации диска. Метаданные — фиксированного размера (порядок сотен мегабайт, например ~200 МБ), потому что записи там фиксированной структуры.

Сами партиции получаются из разбиения диска на равные части (на 50; это решение потом захотелось пересмотреть). При объёмах уровня 100+ ТБ «крупных» контейнеров всё равно получается очень много.

Мы сознательно не пытаемся «разделить мир» на «маленькие файлы» и «большие файлы» на уровне API — всё хранится одинаково. А вот внутри пайплайна есть правило: мелкие куски сначала попадают на SSD. Если писать их на HDD «как есть», random I/O быстро убивает производительность.

Ключевая мысль: **фишка не в том, чтобы просто сложить в контейнер**, а в том, чтобы сохранить/восстановить порядок и читать/писать последовательно. Именно поэтому merge и раскладка так важны.

### Про файловую систему и RAID

**Вопрос**: Почему вы ушли от nginx в сторону самописного решения — из‑за функциональности или производительности?

**Ответ**: Проблема была не только (и не столько) в HTTP‑части. Нам нужно было обеспечить нужную модель хранения на диске и свою логику доступа к контенту (подписи, TTL, лимиты, метаданные) — и проще оказалось строить это как единое решение.

Плюс мы «наивно» рассчитывали, что встроенный TLS в Go нас вывезет. Он удобный по API (например, можно менять сертификаты на лету), но по производительности в нашей задаче оказался узким местом.

**Вопрос**: А RAID используете?

**Ответ**: RAID не используем.

### Про «везёт» и статистику

**Вопрос**: Есть ли у вас статистика «как часто везёт / не везёт», и что происходит в моменты, когда «не повезло»?

**Ответ**: Прямой метрики «везёт/не везёт» у нас нет. Как правило, это выглядит как деградация, а не как полный отказ: серверов много, и система может перераспределяться.

Если клиент «выжирает» полосу на группе edge, в крайних случаях можно отказаться от шардинга и временно раздавать данные более широко, чтобы сгладить пик.

Многое зависит от баланса чтения/записи. Читать с диска можно быстро, но одновременно быстро писать туда же — уже сильно сложнее. Поэтому мы смотрим в сторону более нативного/асинхронного I/O. И да: когда начинается активная запись (merge, восстановление после отказов), отдача может слегка замедляться — но текущая латентность всё равно лучше, чем «раньше, до всех оптимизаций».

### Про переход на Rust

**Вопрос**: Вы упомянули, что хотите заменить Go на Rust. Почему?

**Ответ**: Если выбирать язык «на каждый день», я часто выберу Go — он проще и быстрее даёт результат. Rust сложнее и требует больше усилий.

Но для этой задачи хочется жёсткого контроля над памятью и отсутствия GC, а также возможности глубже уйти в нативное I/O (в том числе io_uring для TCP и файлов). Мы пробовали и бенчмарки, и разные подходы, но из‑за высокой конкуренции запросов и больших объёмов трафика решения, которые выглядят хорошо «на бумаге», не всегда выигрывают в реальности — появляются накладные расходы на взаимодействие.

Если переписывать, то в первую очередь — дисковую часть. А HTTP‑раздачу поверх этого прикрутить уже не так сложно.

### Про sharding и отказоустойчивость

**Вопрос**: Как устроен шардинг? Что происходит при падении диска/сервера?

**Ответ**: Шардинг самописный. Используем consistent hashing, чтобы выбрать «правильный» сервер для объекта.

Если ломается диск, это выглядит как cache miss и деградация. Если «повезло», на диске было мало востребованного — влияние минимально. Если «не повезло», там мог оказаться тёплый/горячий контент — тогда нагрузка перераспределяется, и деградация заметнее.

Когда теряем диск, может резко вырасти запись (восстановление/перераспределение) — это и даёт деградацию. Аналогично при падении сервера: какое-то время мы пытаемся «дожать» подключение, но быстро сдаёмся и перестраиваемся на оставшиеся узлы.

### Про фрагментацию

**Вопрос**: А фрагментация при перезаписи «удалённых» данных — это проблема?

**Ответ**: Пока не видим. Периодически проверяем вручную и снимаем отдельные метрики, но признаём: мониторим не всё, поэтому продолжаем наблюдать.

## Заключение

Мы работаем с HDD, и чтобы их «победить», пришлось много читать, пробовать и консультироваться с людьми. И да — по времени тоже удалось уложиться.

Основные выводы из нашего опыта:

- **Проблему миллиардов мелких файлов можно решить** — упаковка в большие контейнеры с метаданными работает эффективно
- **HDD могут быть эффективны** — при правильной организации данных и последовательной записи можно достичь хорошей производительности
- **Go имеет ограничения** — особенно в работе с памятью и TLS, но для многих задач подходит отлично
- **Метрики критичны** — без них невозможно понять, что происходит в системе и где реальные проблемы
- **Ограничения помогают** — они заставляют думать о правильной архитектуре и не реализовывать лишнее

Если у нас получилось — получится и у вас. Обычно самые неприятные проблемы оказываются довольно базовыми: данные должны быть **отсортированы**, чтобы по ним можно было быстро искать; структуры — **фиксированного размера**, чтобы эффективно хранить их на диске/в памяти и быстро доставать.

Любое решение должно быть аргументировано — нужно понимать, что именно вы делаете и почему. Нерешаемых задач почти не бывает: вопрос в цене и времени. Иногда это не «день работы», а месяц или даже год — и это нормально, если ценность достаточно высокая.

Если ценность решения есть, его можно делать и два года. А если что-то не работает — это не всегда означает, что «вы плохой инженер». Но профилировать всё равно придётся: можно иметь огромный опыт и ошибиться на простейшем — это нормально.

Нас регулярно спасают метрики: когда их много и они правильные, «косяки» видно быстро. В Go с профилированием тоже всё хорошо — это реально помогает.

И последнее: «все врут». В статьях, докладах и бенчмарках часто бывает неправда — иногда от начала и до конца. Поэтому всё нужно проверять под своей нагрузкой: нельзя просто «поставить и поехать» — косяки всё равно всплывут.

Почти всегда окажется, что реальность отличается от описания. И опыт здесь — это понимание, что внедряя любую новую штуку, вы вполне можете выстрелить себе в ногу — а потом всё равно придётся разбираться и чинить.

## Сноски

[^ktls-details]: Подробнее о kTLS, как он работает и как его использовать в Go для достижения высоких скоростей, см. [«Разгоняем Go TLS до 100 Gbps»]({{< relref "talks/razgonyaem-go-tls-do-100-gbps.md" >}}).

[^zero-copy-splice]: О zero-copy механизмах (`sendfile`, `splice`) и их применении для раздачи контента см. [«Разгоняем Go TLS до 100 Gbps»]({{< relref "talks/razgonyaem-go-tls-do-100-gbps.md" >}}) и [«Нет времени объяснять, программируй!»]({{< relref "talks/net-vremeni-obyasnyat-programmiruy.md" >}}).

